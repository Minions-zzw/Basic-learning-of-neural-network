{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.utils.data as tud\n",
    "import pdb\n",
    "import random\n",
    "import struct\n",
    "import gzip\n",
    "\n",
    "if torch.cuda.is_available(): #判断是否有gpu，就是看cuda能不能用\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode\n",
    "landmarks_frame = pd.read_csv(\"../data/project2_train.csv\")  #初步读取文件\n",
    "dataset = landmarks_frame.values   \n",
    "#把表格的数据直接放入变量中\n",
    "#此时dataset的形状为[28706,2] 28706行，2列\n",
    "#第一列是lable类型为int 第二列是data，是一整个字符串，里面包含图像像素值，用空格隔开，类型是string\n",
    "data = dataset[:,1:]  #第二列数据取出来，此时data形状为[28706,1]\n",
    "lable = dataset[:,:1] #第一列lable取出来 此时lable形状为[28706,1]\n",
    "\n",
    "def to_int(x):   #此函数用于把string字符串转换成float的list\n",
    "    return [float(a) for a in x[0].split()]  #split函数可以把string分割开\n",
    "\n",
    "data = [to_int(x) for x in data]  #把string的data变成了形状为[28706,2304]\n",
    "data = np.array(data)   #把list转换成numpy类型的矩阵，形状还是[28706,2304]为之后的操作做准备\n",
    "datas = [[x.reshape(48,48)] for x in data]  \n",
    "#把data中的数据变成[48*48]类型的数据，此时datas形状为[28709,1,48,48]\n",
    "#这种数据形状是最终我们需要的形状\n",
    "#含义为28709个数据，1维，大小为48*48\n",
    "datas = np.array(datas) #datas的类型从list转换为numpy\n",
    "lable = np.array(lable) #lable的类型从list转换为numpy\n",
    "lable = lable.astype(np.int) #把lable里面数据强转为int\n",
    "datas = datas.astype(np.float32) #把atas里面数据强转为float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28709, 1, 48, 48]) torch.Size([28709])\n",
      "(28709, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "training_image = torch.from_numpy(datas) #把datas变成torch类型的矩阵(torch可以使用gpu)\n",
    "training_lable = torch.from_numpy(lable) #同理\n",
    "training_lable = training_lable.squeeze(1)\n",
    "#这一句是把lable减少一维，本来training_lable的形状是[28709,1]\n",
    "#运行完这一句之后就变成了[28709]\n",
    "\n",
    "#最终完成的数据的初步处理\n",
    "#training_image形状为[28709,1,48,48]\n",
    "#training_lable形状为[28709] \n",
    "#都是torch类型的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.utils.data as tud\n",
    "import pdb\n",
    "import random\n",
    "import struct\n",
    "import gzip\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode\n",
    "landmarks_frame = pd.read_csv(\"../data/project2_train.csv\")\n",
    "dataset = landmarks_frame.values\n",
    "data = dataset[:,1:]\n",
    "lable = dataset[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-acb3b2fb6efe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "training_image = training_image[:2000]\n",
    "training_lable = training_lable[:2000]\n",
    "print(training_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来使用dataset将数据封装成batch(成组)\n",
    "#dataset类有三个重要函数，\n",
    "#__init__初始化 在创建此类的时候魂运行一次，用于将数据读入类中\n",
    "#__len__测量大小，用于和下面那个函数配合使用\n",
    "#__getitem__，此函数是dataset可以生成batch的关键函数，此函数的返回值代表取出一个数据。\n",
    "#则此类会成组的调用__getitem__来生成数据，把每一个返回值拼装成成组的数据\n",
    "#例如，batch_size设定为40，则此类就会成组的运行40次__getitem__来返回数据，每40次把数据合成一组\n",
    "#一共运行__len__返回值除以40次，也就是生成__len__返回值除以40组的数据\n",
    "class ExpressionClassificationDataset(tud.Dataset): #定义数据读取类，此类用于读取数据，最为关键的函数是getitem（）\n",
    "    def __init__(self,training_image,training_lable, CEL): #此函数用于初始化\n",
    "        super(ExpressionClassificationDataset, self).__init__()\n",
    "        self.training_data = training_image.to(DEVICE)\n",
    "        if(CEL == True): #CEL 代表CrossEntropyLoss 此loss方程需要需要的结果y和其他loss方程不一样（单维），所以需要另外处理\n",
    "            self.training_lable = training_lable.to(DEVICE)\n",
    "        else:\n",
    "            self.training_lable = torch.zeros(len(training_lable),7,device = DEVICE)\n",
    "            for i in range(len(training_lable)):\n",
    "                self.training_lable[i][training_lable[i]] = 1\n",
    "        \n",
    "    def __len__(self): #此函数用于确定总数据大小，用于生成所有的bitch\n",
    "        return len(self.training_data)\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.training_data[idx]/255.,self.training_lable[idx]; #/255是把数据的大小限制在0-1之间，能极大减少学习的时间\n",
    "            \n",
    "        \n",
    "#此类的具体用下在下一运行格中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#示例如下，读取training_image和training_lable\n",
    "dataset = ExpressionClassificationDataset(training_image,training_lable,CEL = CEL)#用training_image和training_lable生成dataset类\n",
    "train_loader = tud.DataLoader(dataset,batch_size = 40, shuffle = False)#把数据提取成组,shuffle的意思是是否随机抽取\n",
    "#这一句所生成的train_loader的结构为[718,[40,1,48,48],[40]]\n",
    "#调用方法是 for i,(x,y) in enumerate (train_loader): #每次取出的x形状为[40,1,48,48],y形状为[40]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------用50行代码搭建ResNet-------------------------------------------\n",
    "import torch as t\n",
    " \n",
    "class ResidualBlock(nn.Module):\n",
    "    #实现子module: Residual    Block\n",
    "    def __init__(self,inchannel,outchannel,stride=1,shortcut=None):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.left=nn.Sequential(\n",
    "            nn.Conv2d(inchannel,outchannel,3,stride,1,bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel,outchannel,3,1,1,bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        \n",
    "        self.right=shortcut\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.left(x)\n",
    "        residual=x if self.right is None else self.right(x)\n",
    "        out+=residual\n",
    "        return F.relu(out)\n",
    "    \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    #实现主module:ResNet34\n",
    "    #ResNet34包含多个layer,每个layer又包含多个residual block\n",
    "    #用子module实现residual block , 用 _make_layer 函数实现layer\n",
    "    def __init__(self,num_classes=1000):\n",
    "        super(ResNet,self).__init__()\n",
    "        #重复的layer,分别有3,4,6,3个residual block\n",
    "        self.BN = nn.BatchNorm2d(1)\n",
    "        self.layer1=self._make_layer(1,64,3)\n",
    "        self.layer2=self._make_layer(64,128,4,stride=2)\n",
    "        self.layer3=self._make_layer(128,256,6,stride=2)\n",
    "        self.layer4=self._make_layer(256,512,3,stride=2)\n",
    "        \n",
    "        #分类用的全连接\n",
    "        self.fc=nn.Linear(512,num_classes)\n",
    "        \n",
    "    def _make_layer(self,inchannel,outchannel,block_num,stride=1):\n",
    "        #构建layer,包含多个residual block\n",
    "        shortcut=nn.Sequential(\n",
    "            nn.Conv2d(inchannel,outchannel,1,stride,bias=False),\n",
    "            nn.BatchNorm2d(outchannel))\n",
    " \n",
    "        layers=[ ]\n",
    "        layers.append(ResidualBlock(inchannel,outchannel,stride,shortcut))\n",
    "        \n",
    "        for i in range(1,block_num):\n",
    "            layers.append(ResidualBlock(outchannel,outchannel))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.BN(x)\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        x=F.avg_pool2d(x,6)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,20,3) \n",
    "#         48-3+1 = 46\n",
    "#         23-3+1 = 21\n",
    "#         21-2+1 = 20\n",
    "        #pooling\n",
    "        self.conv2 = torch.nn.Conv2d(20,20,3) \n",
    "        self.conv3 = torch.nn.Conv2d(20,20,2) \n",
    "        self.linear1 = torch.nn.Linear(20*10*10,1000)\n",
    "        self.linear4 = torch.nn.Linear(1000,500)\n",
    "        self.linear5 = torch.nn.Linear(500,200)\n",
    "        self.linear6 = torch.nn.Linear(200,100)\n",
    "        self.linear7 = torch.nn.Linear(100,7)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         print(x[0][1])\n",
    "#         pdb.set_trace()\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1,20*10*10)\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        x = F.relu(self.linear6(x))\n",
    "        x = F.relu(self.linear7(x))\n",
    "#         print(F.softmax(x, dim = 1)) #调试语句\n",
    "#         pdb.set_trace()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cfb2bb1310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "\n",
    "# for i,(x,y) in enumerate (train_loader):\n",
    "#     if i<1:\n",
    "#         print(x)\n",
    "#         print(y)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_number,test_loader): #测试函数\n",
    "    total_correct = 0.0\n",
    "    for i, (x,y) in enumerate(test_loader):\n",
    "        y_pred = model.forward(x)  #前项传播\n",
    "        correct = sum(a == torch.argmax(b) for a, b in zip(y,y_pred))\n",
    "        total_correct = total_correct+correct\n",
    "#             pdb.set_trace()\n",
    "        \n",
    "    return total_correct/test_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :轮，loss =  1.9381694793701172\n",
      "准确率为： tensor(0.2350)\n",
      "1 :轮，loss =  1.8617029190063477\n",
      "准确率为： tensor(0.2350)\n",
      "2 :轮，loss =  1.8915783166885376\n",
      "准确率为： tensor(0.2350)\n",
      "3 :轮，loss =  1.9033114910125732\n",
      "准确率为： tensor(0.2350)\n",
      "4 :轮，loss =  1.9873697757720947\n",
      "准确率为： tensor(0.2350)\n",
      "5 :轮，loss =  1.8686262369155884\n",
      "准确率为： tensor(0.2350)\n",
      "6 :轮，loss =  1.953998327255249\n",
      "准确率为： tensor(0.2350)\n",
      "7 :轮，loss =  1.8475284576416016\n",
      "准确率为： tensor(0.2350)\n",
      "8 :轮，loss =  1.9234898090362549\n",
      "准确率为： tensor(0.2350)\n",
      "9 :轮，loss =  1.8503059148788452\n",
      "准确率为： tensor(0.2350)\n",
      "10 :轮，loss =  1.9596607685089111\n",
      "准确率为： tensor(0.2350)\n",
      "11 :轮，loss =  1.8871047496795654\n",
      "准确率为： tensor(0.2350)\n",
      "12 :轮，loss =  1.9606313705444336\n",
      "准确率为： tensor(0.2350)\n",
      "13 :轮，loss =  1.812024712562561\n",
      "准确率为： tensor(0.2350)\n",
      "14 :轮，loss =  1.8859474658966064\n",
      "准确率为： tensor(0.2350)\n",
      "15 :轮，loss =  1.8862431049346924\n",
      "准确率为： tensor(0.2350)\n",
      "16 :轮，loss =  1.8120044469833374\n",
      "准确率为： tensor(0.2350)\n",
      "17 :轮，loss =  1.9231780767440796\n",
      "准确率为： tensor(0.2350)\n",
      "18 :轮，loss =  1.925299882888794\n",
      "准确率为： tensor(0.2350)\n",
      "19 :轮，loss =  1.8865677118301392\n",
      "准确率为： tensor(0.2350)\n",
      "20 :轮，loss =  1.8104320764541626\n",
      "准确率为： tensor(0.2350)\n",
      "21 :轮，loss =  1.9614843130111694\n",
      "准确率为： tensor(0.2350)\n",
      "22 :轮，loss =  1.8477535247802734\n",
      "准确率为： tensor(0.2350)\n",
      "23 :轮，loss =  2.037142753601074\n",
      "准确率为： tensor(0.2350)\n",
      "24 :轮，loss =  1.9236271381378174\n",
      "准确率为： tensor(0.2350)\n",
      "25 :轮，loss =  1.9237552881240845\n",
      "准确率为： tensor(0.2350)\n",
      "26 :轮，loss =  1.8860574960708618\n",
      "准确率为： tensor(0.2350)\n",
      "27 :轮，loss =  1.9989954233169556\n",
      "准确率为： tensor(0.2350)\n",
      "28 :轮，loss =  1.697567343711853\n",
      "准确率为： tensor(0.2350)\n",
      "29 :轮，loss =  1.961846113204956\n",
      "准确率为： tensor(0.2350)\n"
     ]
    }
   ],
   "source": [
    "CEL = True\n",
    "eta = 0.01\n",
    "batch_size = 10\n",
    "NUM_EPOCHS = 30\n",
    "n_train = len(training_image)\n",
    "split = n_train // 10   #整数除法，要把数据分成9:1的训练集和测试集\n",
    "test_number = split  #测试集总数\n",
    "indices = list(range(n_train))\n",
    "train_sampler = tud.sampler.SubsetRandomSampler(indices[split:])  #设置采样器\n",
    "test_sampler = tud.sampler.SubsetRandomSampler(indices[:split])\n",
    "dataset = ExpressionClassificationDataset(training_image,training_lable,CEL = CEL)\n",
    "train_loader = tud.DataLoader(dataset, sampler=train_sampler,batch_size = batch_size, shuffle = False)#利用采样的方法进行提取\n",
    "test_loader = tud.DataLoader(dataset, sampler=test_sampler, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "model = Net()\n",
    "loss_fn = nn.CrossEntropyLoss() #定义loss_fn，loss方程\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = eta)#定义参数更新方式\n",
    "for e in range(NUM_EPOCHS): #每一个epochs\n",
    "    for i,(x,y) in enumerate (train_loader):\n",
    "#从dataloader中取出bitch数据进行训练\n",
    "#               a = x[0].reshape(48,48) #显示图片\n",
    "#               fig = plt.figure()\n",
    "#               plotwindow = fig.add_subplot(111)\n",
    "#               plt.imshow(a , cmap='gray')\n",
    "#               plt.show()\n",
    "#               print(y[0])\n",
    "        optimizer.zero_grad() #参数更新器重置为0\n",
    "        y_pred = model.forward(x)  #前项传播 \n",
    "#         if(i % 5 ==0):\n",
    "#             print(y_pred) \n",
    "        loss = loss_fn(y_pred,y.long()) #计算loss\n",
    "        loss.backward()   # 反向传播\n",
    "        optimizer.step() #参数更新\n",
    "#     print(y_pred)\n",
    "    print(e,\":轮，loss = \",loss.item())\n",
    "    print(\"准确率为：\",test(model,test_number,test_loader));\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionClassification(object):\n",
    "    def __init__(self,training_image,training_lable,eta,batch_size,shuffle,CEL): #CEL 代表使用交叉熵计算loss\n",
    "        self.CEL = CEL\n",
    "        self.dataset = ExpressionClassificationDataset(training_image,training_lable,CEL = self.CEL)\n",
    "        self.split_9_1(len(training_image),batch_size) #把数据分为训练集和测试集  详细过程见下面的那个split函数\n",
    "        self.dataloader = tud.DataLoader(self.dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = ResNet(7).cuda()  #定义模型类\n",
    "        else:\n",
    "            self.model = ResNet(7)\n",
    "        if (CEL == True):\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss() #定义loss_fn，loss方程\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.MSELoss() #定义loss_fn，loss方程\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr = eta)#定义参数更新方式\n",
    "    def train(self): #开始训练\n",
    "        for e in range(NUM_EPOCHS): #每一个epochs\n",
    "            for i,(x,y) in enumerate (self.train_loader): #从dataloader中取出bitch数据进行训练\n",
    "#                 a = x[0].reshape(28,28) #显示图片\n",
    "#                 fig = plt.figure()\n",
    "#                 plotwindow = fig.add_subplot(111)\n",
    "#                 plt.imshow(a , cmap='gray')\n",
    "#                 plt.show()\n",
    "#                 print(y[0])\n",
    "                y_pred = self.model.forward(x)  #前项传播\n",
    "                \n",
    "                \n",
    "                if self.CEL == True:\n",
    "                    self.loss = self.loss_fn(y_pred,y.long()) #计算loss\n",
    "                else:\n",
    "                    self.loss = self.loss_fn(y_pred,y) #计算loss\n",
    "                if(i % 5 ==0):\n",
    "                    print(y_pred)\n",
    "#                 print(self.loss)\n",
    "                print(self.loss)\n",
    "                self.optimizer.zero_grad() #参数更新器重置为0\n",
    "                self.loss.backward()   # 反向传播\n",
    "                self.optimizer.step() #参数更新\n",
    "                \n",
    "            print(e,\":轮，loss = \",self.loss.item())\n",
    "            print(\"准确率为：\",self.test());\n",
    "    def test(self): #测试函数\n",
    "        total_correct = 0.0\n",
    "        for i, (x,y) in enumerate(self.test_loader):\n",
    "            y_pred = self.model.forward(x)  #前项传播\n",
    "            if self.CEL == True:  #对于不同loss方程的处理\n",
    "                correct = self.correct_count_CEL(y,y_pred)\n",
    "            else:\n",
    "                correct = self.correct_count_MSE(y,y_pred)\n",
    "\n",
    "            total_correct = total_correct+correct\n",
    "#             pdb.set_trace()\n",
    "        \n",
    "        return total_correct/self.test_number\n",
    "    def correct_count_MSE(self,y,y_pred): #MES正确预测数量函数\n",
    "        return sum(a[torch.argmax(b)] == 1 for a, b in zip(y,y_pred))\n",
    "    def correct_count_CEL(self,y,y_pred): #CrossEntropyLoss正确预测数量函数\n",
    "        return sum(a == torch.argmax(b) for a, b in zip(y,y_pred))\n",
    "    def split_9_1(self,n_train,batch_size):\n",
    "        split = n_train // 10   #整数除法，要把数据分成9:1的训练集和测试集\n",
    "        self.test_number = split  #测试集总数\n",
    "        indices = list(range(n_train))\n",
    "        train_sampler = tud.sampler.SubsetRandomSampler(indices[split:])  #设置采样器\n",
    "        test_sampler = tud.sampler.SubsetRandomSampler(indices[:split])\n",
    "        self.train_loader = tud.DataLoader(self.dataset, sampler=train_sampler,batch_size = batch_size, shuffle=False)#利用采样的方法进行提取\n",
    "        self.test_loader = tud.DataLoader(self.dataset, sampler=test_sampler, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1072,  0.1080,  0.3656, -0.5559,  0.8535, -0.1481,  0.5055],\n",
      "        [-0.3731, -0.0215,  0.4637, -0.4113,  0.7316, -0.2247,  0.7401],\n",
      "        [-0.0631, -0.0797,  0.5682, -0.3058,  0.7862, -0.1177,  0.6846],\n",
      "        [-0.3286,  0.3317,  0.4767, -0.1870,  0.8966,  0.2834,  0.4463],\n",
      "        [-0.2926,  0.0886,  0.4207, -0.1479,  0.7421, -0.1126,  0.6714],\n",
      "        [-0.2461,  0.0400,  0.2240, -0.2032,  0.8572,  0.1783,  0.6081],\n",
      "        [-0.4260,  0.3748,  0.3977, -0.0313,  0.7176,  0.2455,  0.6103],\n",
      "        [-0.1150,  0.1483,  0.2892, -0.2626,  0.8061,  0.1230,  0.6754],\n",
      "        [-0.2013,  0.1650,  0.4929, -0.1216,  0.6711,  0.1234,  0.6325],\n",
      "        [-0.0915,  0.2254,  0.5042, -0.0584,  0.5878,  0.2417,  0.7225],\n",
      "        [-0.2995, -0.1190,  0.4811, -0.2755,  0.8314, -0.1850,  0.6385],\n",
      "        [-0.1505,  0.3400,  0.2787, -0.0673,  0.6915,  0.3559,  0.6501],\n",
      "        [-0.2859,  0.3486,  0.1959,  0.0484,  0.8756,  0.1914,  0.6920],\n",
      "        [-0.3199,  0.2640,  0.3216, -0.1047,  0.7695,  0.4390,  0.6000],\n",
      "        [-0.0757,  0.2876,  0.4300, -0.1702,  0.8051,  0.1735,  0.7656],\n",
      "        [-0.2345,  0.3025,  0.3789, -0.0367,  0.7994,  0.4487,  0.6657],\n",
      "        [-0.3345,  0.2143,  0.2937, -0.2593,  0.8899,  0.2799,  0.6873],\n",
      "        [-0.2644,  0.2881,  0.2090, -0.1074,  0.7673,  0.2997,  0.6851],\n",
      "        [-0.1418, -0.0631,  0.2888, -0.0113,  0.8757,  0.0995,  0.5124],\n",
      "        [-0.4275,  0.1888,  0.2761,  0.0840,  0.8619,  0.1621,  0.7981],\n",
      "        [-0.2540,  0.1973,  0.4231, -0.2222,  0.9692,  0.4276,  0.6680],\n",
      "        [-0.2556,  0.1842,  0.4175, -0.1984,  0.8353,  0.2024,  0.6531],\n",
      "        [-0.2175,  0.1742,  0.3405,  0.0501,  0.6209,  0.1067,  0.6045],\n",
      "        [-0.2164,  0.1087,  0.3291, -0.2938,  0.6373,  0.0988,  0.5818],\n",
      "        [-0.3057,  0.0197,  0.3424, -0.1360,  0.8758,  0.1515,  0.8275],\n",
      "        [ 0.0350, -0.2395,  0.4976, -0.6842,  0.5899, -0.2618,  0.5435],\n",
      "        [-0.3528,  0.2448,  0.1990, -0.1562,  0.8372,  0.1604,  0.7313],\n",
      "        [-0.0833, -0.0244,  0.5607, -0.1486,  0.7084, -0.1675,  0.7498],\n",
      "        [-0.3401,  0.2605,  0.3267,  0.0802,  0.7541,  0.4810,  0.6202],\n",
      "        [-0.3829, -0.1310,  0.5548, -0.4047,  0.8067, -0.0688,  0.8650],\n",
      "        [-0.0829, -0.1103,  0.4887, -0.6297,  0.8636, -0.2623,  0.7552],\n",
      "        [-0.1072, -0.3122,  0.6718, -1.0646,  1.0586, -0.6818,  0.8188],\n",
      "        [-0.2174,  0.2612,  0.3726, -0.3655,  0.6918,  0.2233,  0.6001],\n",
      "        [-0.2719,  0.1798,  0.3607, -0.2008,  0.7435,  0.1474,  0.7557],\n",
      "        [-0.2231,  0.3338,  0.3480, -0.0983,  0.8515,  0.1866,  0.7384],\n",
      "        [-0.4589,  0.3303,  0.3487, -0.0526,  0.8976,  0.3451,  0.7657],\n",
      "        [-0.0950, -0.0674,  0.6745, -0.4913,  0.7353, -0.5335,  0.7734],\n",
      "        [-0.1700,  0.2550,  0.3030, -0.1651,  0.7252,  0.2706,  0.6878],\n",
      "        [-0.3084,  0.0962,  0.3473, -0.1885,  0.7344, -0.1157,  0.7034],\n",
      "        [ 0.1497, -0.2433,  0.8994, -0.7731,  0.9673, -0.6893,  0.5149],\n",
      "        [-0.2602,  0.0692,  0.2999, -0.2372,  0.5259,  0.2525,  0.6306],\n",
      "        [-0.1717,  0.0867,  0.3303, -0.3091,  0.8215, -0.0476,  0.9127],\n",
      "        [-0.1746,  0.3562,  0.2963, -0.1494,  0.6166,  0.1593,  0.6527],\n",
      "        [-0.0481,  0.0858,  0.4146, -0.2743,  0.7606, -0.1494,  0.7738],\n",
      "        [-0.2690,  0.2490,  0.3249, -0.1253,  0.7385,  0.3136,  0.6114],\n",
      "        [-0.3143,  0.0943,  0.4291, -0.1313,  0.8710,  0.2378,  0.8532],\n",
      "        [-0.0029, -0.1633,  0.5970, -0.3403,  0.8163, -0.1221,  0.7124],\n",
      "        [-0.3237,  0.2256,  0.3994, -0.1980,  0.8124, -0.1362,  0.5832],\n",
      "        [-0.2715,  0.0599,  0.3605, -0.0559,  0.7955,  0.3088,  0.5540],\n",
      "        [-0.2165, -0.0018,  0.3944, -0.3273,  0.8697,  0.1771,  0.4410],\n",
      "        [ 0.1139, -0.1810,  0.8617, -0.7292,  1.0904, -0.7729,  0.9400],\n",
      "        [-0.1357, -0.0457,  0.4765, -0.2840,  0.9201,  0.0497,  0.4899],\n",
      "        [-0.1496,  0.1073,  0.3142, -0.2459,  0.6526,  0.1757,  0.9004],\n",
      "        [-0.1938,  0.1887,  0.3642, -0.1893,  0.6743,  0.0255,  0.6932],\n",
      "        [-0.0612,  0.0696,  0.4683, -0.4049,  0.7422, -0.0067,  0.5689],\n",
      "        [-0.4304,  0.2092,  0.0892, -0.0775,  1.0107,  0.1095,  0.6894],\n",
      "        [-0.3246,  0.3316,  0.2550, -0.0802,  0.9354,  0.2925,  0.8796],\n",
      "        [-0.2634,  0.1663,  0.1304, -0.4163,  0.6817, -0.1203,  0.6423],\n",
      "        [-0.3250,  0.2997,  0.3870, -0.0209,  0.8230,  0.3277,  0.7937],\n",
      "        [-0.0954,  0.1896,  0.2806, -0.1900,  0.9160,  0.0746,  0.5638],\n",
      "        [ 0.0179,  0.0806,  0.6388, -0.5530,  0.9423, -0.3006,  0.6888],\n",
      "        [-0.4391, -0.1088,  0.6232, -0.0706,  0.8528,  0.0660,  0.8231],\n",
      "        [-0.1264,  0.1656,  0.3946, -0.2586,  0.7342, -0.0186,  0.6766],\n",
      "        [-0.2596,  0.1984,  0.1901, -0.1434,  0.7749,  0.0859,  0.7710]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(2.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8297, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0089, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8277, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.6480, -0.9653,  0.1871,  1.0931,  0.5482,  0.2468,  0.3781],\n",
      "        [ 0.5301, -1.4234,  1.0512,  1.5910, -0.8319,  0.3525,  0.1675],\n",
      "        [ 0.4127, -1.0887,  0.3983,  1.3051,  0.1118,  0.2166,  0.1559],\n",
      "        [ 0.5000, -1.1335,  0.3687,  1.2134,  0.4138,  0.2400,  0.3784],\n",
      "        [ 0.5854, -1.3012,  0.2162,  1.1070,  0.1695,  0.1451,  0.1106],\n",
      "        [ 0.3220, -1.0318,  0.1343,  1.1051,  0.4581,  0.1565,  0.3451],\n",
      "        [ 0.3460, -1.1121,  0.2215,  1.0275,  0.4340,  0.3288,  0.1469],\n",
      "        [ 0.4769, -1.1539,  0.4007,  1.0600,  0.2721,  0.2258,  0.3486],\n",
      "        [ 0.3362, -1.0603,  0.1136,  1.3216,  0.4291,  0.4658,  0.0667],\n",
      "        [ 0.5783, -1.2987,  0.3550,  1.0956, -0.1851,  0.1742,  0.2464],\n",
      "        [ 0.5904, -1.3093,  0.3596,  1.1155, -0.0324,  0.1380,  0.4459],\n",
      "        [ 0.4039, -0.9863,  0.1519,  1.1795,  0.5525,  0.4178,  0.2688],\n",
      "        [ 0.4542, -1.3173,  0.6516,  1.1631, -0.2257,  0.2621,  0.3561],\n",
      "        [ 0.5768, -1.3082,  0.4725,  1.2803, -0.0652,  0.4534,  0.3912],\n",
      "        [ 0.5310, -1.0540,  0.3474,  1.2279,  0.1952,  0.2548,  0.1795],\n",
      "        [ 0.4974, -1.0173,  0.1787,  1.1686,  0.4921,  0.2818,  0.0507],\n",
      "        [ 0.3810, -1.5981,  1.0929,  1.3026, -0.7244,  0.2438,  0.3240],\n",
      "        [ 0.3362, -1.0023,  0.3967,  1.0008,  0.0180,  0.1973,  0.0567],\n",
      "        [ 0.4797, -1.0718,  0.3394,  1.2383,  0.5681,  0.1851,  0.1957],\n",
      "        [ 0.4339, -0.9077,  0.2058,  1.1302,  0.3220,  0.3738,  0.1584],\n",
      "        [ 0.4796, -0.9498,  0.1959,  1.2378,  0.5335,  0.3664,  0.0563],\n",
      "        [ 0.2958, -1.4192,  0.7919,  1.5903, -0.2670,  0.0652,  0.3884],\n",
      "        [ 0.4026, -0.9367,  0.0893,  1.2581,  0.6204,  0.2246,  0.1757],\n",
      "        [ 0.2166, -1.3177,  0.4201,  1.1622,  0.0297,  0.1219,  0.2347],\n",
      "        [ 0.4947, -0.9079,  0.1359,  1.1905,  0.4146,  0.2633,  0.2467],\n",
      "        [ 0.6676, -0.9759,  0.2729,  1.2649,  0.3165,  0.2474,  0.3372],\n",
      "        [ 0.4210, -1.2417,  0.2169,  1.1387,  0.5459,  0.4124,  0.1216],\n",
      "        [ 0.3477, -1.3845,  0.7269,  1.3696, -0.1760,  0.4787,  0.1227],\n",
      "        [ 0.4503, -0.9782,  0.2312,  1.1790,  0.4951,  0.1808,  0.0980],\n",
      "        [ 0.4613, -1.2040,  0.3565,  1.1283,  0.1787,  0.0246,  0.1957],\n",
      "        [ 0.4697, -0.9722,  0.2800,  1.2127,  0.4250,  0.2126,  0.2173],\n",
      "        [ 0.4652, -0.9795,  0.2554,  1.2382,  0.4349,  0.4586,  0.1511],\n",
      "        [ 0.4058, -0.9357,  0.3661,  1.1957,  0.3213,  0.3239,  0.2940],\n",
      "        [ 0.4134, -1.1396,  0.4710,  1.2181,  0.2248,  0.2501,  0.2954],\n",
      "        [ 0.5510, -1.0448,  0.3085,  0.9496,  0.4993,  0.3915,  0.2466],\n",
      "        [ 0.3922, -1.6327,  0.6770,  1.2193, -0.2710,  0.1473,  0.2706],\n",
      "        [ 0.5227, -1.3867,  0.4205,  1.2214,  0.0757,  0.2370,  0.1305],\n",
      "        [ 0.3575, -0.9276,  0.2342,  1.2428,  0.3394,  0.1582,  0.3985],\n",
      "        [ 0.4951, -1.1150,  0.2226,  1.1389,  0.6104,  0.3343,  0.3041],\n",
      "        [ 0.1432, -1.4790,  0.9532,  1.2882, -0.5532,  0.2984,  0.1918],\n",
      "        [ 0.4624, -0.9697,  0.0455,  1.1426,  0.5132,  0.2105,  0.1941],\n",
      "        [ 0.6674, -1.1206,  0.1817,  1.1570,  0.3346,  0.2167,  0.1772],\n",
      "        [ 0.5523, -1.0491,  0.3834,  1.2027,  0.3774,  0.2544,  0.0706],\n",
      "        [ 0.3459, -1.1205,  0.4440,  1.2007,  0.1727,  0.2209,  0.1823],\n",
      "        [ 0.4234, -1.0136,  0.3330,  1.1181,  0.0552,  0.3734,  0.0250],\n",
      "        [ 0.2511, -0.9594,  0.2801,  1.3499,  0.3922,  0.4189,  0.1541],\n",
      "        [ 0.3311, -1.0493,  0.1159,  1.2301,  0.5245,  0.1900,  0.2414],\n",
      "        [ 0.2190, -0.9828,  0.1009,  1.2055,  0.5167,  0.2976,  0.2834],\n",
      "        [ 0.5135, -1.2500,  0.3660,  1.0323,  0.1795,  0.3549,  0.2198],\n",
      "        [ 0.4950, -1.1669,  0.2083,  1.2339,  0.0150,  0.0836,  0.1702],\n",
      "        [ 0.4418, -0.9317,  0.2756,  1.0524,  0.4592,  0.3684,  0.0626],\n",
      "        [ 0.4142, -1.2155,  0.5244,  1.1354,  0.3389,  0.3994,  0.1191],\n",
      "        [ 0.4639, -1.0488,  0.1567,  1.1238,  0.4554,  0.1436,  0.0912],\n",
      "        [ 0.2290, -1.2864,  0.4100,  1.2729,  0.1242,  0.5400,  0.2669],\n",
      "        [ 0.4738, -0.8719,  0.2598,  1.1827,  0.3397,  0.3804,  0.1274],\n",
      "        [ 0.3180, -1.0270,  0.2403,  1.1319,  0.6112,  0.1704,  0.2923],\n",
      "        [ 0.3459, -1.0804,  0.2533,  1.0858,  0.4424,  0.3900,  0.1705],\n",
      "        [ 0.4523, -1.3125,  0.1923,  1.0521,  0.4572,  0.1493,  0.1159],\n",
      "        [ 0.4302, -0.9189,  0.2576,  1.1654,  0.4318,  0.3696,  0.3367],\n",
      "        [ 0.3583, -1.0798,  0.3162,  1.2177,  0.3600,  0.2766,  0.2539],\n",
      "        [ 0.5952, -1.4087,  0.8674,  1.0972, -0.1426,  0.1939,  0.2264],\n",
      "        [ 0.3053, -0.8929,  0.2948,  1.2214,  0.5739,  0.4571,  0.3835],\n",
      "        [ 0.3334, -0.9544,  0.2573,  1.1513,  0.5796,  0.2901,  0.2028],\n",
      "        [ 0.4084, -1.0041,  0.1199,  1.1218,  0.4573,  0.2508,  0.1136]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(1.7660, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8525, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0398, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8246, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8266, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "EC = ExpressionClassification(training_image,training_lable,eta = 0.01,batch_size = 64,shuffle = False,CEL = True)\n",
    "EC.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "EPOCH = 30\n",
    "BITH_SIZE = 50\n",
    "learning_rate = 0.001\n",
    "n = len(training_data)\n",
    "n_test = len(test_data)\n",
    "test_datas = torch.zeros((n_test,1,48,48),device = DEVICE)\n",
    "test_lables = torch.zeros((n_test,7),device = DEVICE)\n",
    "test_datas = test_datas[:1000]\n",
    "test_lables = test_lables[:1000]\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_lable = torch.from_numpy(test_lable)\n",
    "for x in range(1000) :\n",
    "    test_datas[x] = test_data[x]\n",
    "    test_lables[x] = test_lable[x]\n",
    "training_data = torch.from_numpy(training_data)\n",
    "training_lable = torch.from_numpy(training_lable)\n",
    "training_datas = torch.zeros((int(n/BITH_SIZE),BITH_SIZE,1,48,48),device = DEVICE)\n",
    "training_lables = torch.zeros((int(n/BITH_SIZE),BITH_SIZE,7),device = DEVICE)\n",
    "for x,y in zip(range(0,n,BITH_SIZE),range(int(n/BITH_SIZE))):\n",
    "    training_datas[y] = training_data[x:x+BITH_SIZE]\n",
    "    training_lables[y] = training_lable[x:x+BITH_SIZE]\n",
    "# training_datas = training_datas[:1000]  #测试用\n",
    "# training_lables = training_lables[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8b6a5b6c8dea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m#         i = i+1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#         print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 948\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2218\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2219\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,20,3) \n",
    "#         48-3+1 = 46\n",
    "#         23-3+1 = 21\n",
    "#         21-2+1 = 20\n",
    "        #pooling\n",
    "        self.conv2 = torch.nn.Conv2d(20,20,3) \n",
    "        self.conv3 = torch.nn.Conv2d(20,20,2) \n",
    "        self.linear1 = torch.nn.Linear(20*10*10,3000)\n",
    "        self.linear2 = torch.nn.Linear(3000,2000)\n",
    "        self.linear3 = torch.nn.Linear(2000,1000)\n",
    "        self.linear4 = torch.nn.Linear(1000,500)\n",
    "        self.linear5 = torch.nn.Linear(500,200)\n",
    "        self.linear6 = torch.nn.Linear(200,7)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         print(x[0][1])\n",
    "#         pdb.set_trace()\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1,20*10*10)\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        x = self.linear6(x)\n",
    "#         print(F.softmax(x, dim = 1)) #调试语句\n",
    "#         pdb.set_trace()\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "model = Net()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "training_datas = training_datas[:1000]\n",
    "training_lables = training_lables[:1000]\n",
    "for n in range(EPOCH):\n",
    "#     i = 0\n",
    "    for x,y in zip(training_datas,training_lables):\n",
    "\n",
    "        y_pred = model.forward(x)\n",
    "#         print(y.shape)\n",
    "#         print(y_pred)\n",
    "#         pdb.set_trace()\n",
    "    #计算loss\n",
    "#         i = i+1\n",
    "#         print(i)\n",
    "        loss = loss_fn(y_pred, y.long())\n",
    "        print(loss)\n",
    "        optimizer.zero_grad()\n",
    "#         print(y)\n",
    "#         pdb.set_trace()\n",
    "        loss.backward()\n",
    "    \n",
    "        #update model parameters\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    print(n,\"测试结果:\",test())\n",
    "def test():\n",
    "    xxx = torch.zeros(1000,1)\n",
    "    a = model.forward(test_datas)\n",
    "    for k,i in zip(a,range(1000)):\n",
    "        xxx[i] = torch.argmax(k)\n",
    "    \n",
    "    return sum(y[int(x)] == 1 for x, y in zip(xxx,test_lables))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,20,3) \n",
    "#         48-3+1 = 46\n",
    "#         23-3+1 = 21\n",
    "#         21-2+1 = 20\n",
    "        #pooling\n",
    "        self.conv2 = torch.nn.Conv2d(20,20,3) \n",
    "        self.conv3 = torch.nn.Conv2d(20,20,2) \n",
    "        self.linear1 = torch.nn.Linear(20*10*10,3000)\n",
    "        self.linear2 = torch.nn.Linear(3000,2000)\n",
    "        self.linear3 = torch.nn.Linear(2000,1000)\n",
    "        self.linear4 = torch.nn.Linear(1000,500)\n",
    "        self.linear5 = torch.nn.Linear(500,200)\n",
    "        self.linear6 = torch.nn.Linear(200,7)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         print(x[0][1])\n",
    "#         pdb.set_trace()\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1,20*10*10)\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        x = self.linear6(x)\n",
    "#         print(F.softmax(x, dim = 1)) #调试语句\n",
    "#         pdb.set_trace()\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        \n",
    "class FaceNet(object):\n",
    "    def __init__(self,training_data,training_lable,epoch,mini,eta,test_data = None, test_lable = None):\n",
    "        n = len(training_data)\n",
    "        n_test = len(test_data)\n",
    "        self.test_datas = torch.zeros((n_test,1,48,48),device = DEVICE)\n",
    "        self.test_lables = torch.zeros((n_test,7),device = DEVICE)\n",
    "        self.test_datas = self.test_datas[:1000]\n",
    "        self.test_lables = self.test_lables[:1000]\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_lable = torch.from_numpy(test_lable)\n",
    "        for x in range(1000) :\n",
    "            self.test_datas[x] = test_data[x]\n",
    "            self.test_lables[x] = test_lable[x]\n",
    "        \n",
    "        print(n,mini)\n",
    "        training_lable = training_lable[:20000]\n",
    "        training_lable = training_lable.reshape(1,20000)\n",
    "        training_lable = training_lable.astype(float)\n",
    "        training_data = torch.from_numpy(training_data)\n",
    "        training_lable = torch.from_numpy(training_lable)\n",
    "\n",
    "        self.training_datas = torch.zeros((int(n/mini),mini,1,48,48),device = DEVICE)\n",
    "        self.training_lables = torch.zeros((int(n/mini),mini),device = DEVICE)\n",
    "        for x,y in zip(range(0,n,mini),range(int(n/mini))):\n",
    "            self.training_datas[y] = training_data[x:x+mini]\n",
    "            self.training_lables[y] = training_lable[0][x:x+mini]\n",
    "        self.training_datas = self.training_datas[:100]  #测试用\n",
    "        self.training_lables = self.training_lables[:100]\n",
    "        self.epoch = epoch\n",
    "        self.model = Net()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.learning_rate = eta\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = eta)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for n in range(self.epoch):\n",
    "            #前项传播\n",
    "            self.optimizer.zero_grad()\n",
    "            for x,y in zip(self.training_datas,self.training_lables):\n",
    "                y_pred = self.model.forward(x)\n",
    "#                 print(y_pred)\n",
    "#                 pdb.set_trace()\n",
    "                #计算loss\n",
    "                loss = self.loss_fn(y_pred,y.long())\n",
    "                \n",
    "                print(loss)\n",
    "                \n",
    "                loss.backward()\n",
    "            \n",
    "                #update model parameters\n",
    "                self.optimizer.step()\n",
    "            print(n,\"测试结果:\",self.test())\n",
    "    def test(self):\n",
    "        xxx = torch.zeros(1000,1)\n",
    "        a = self.model.forward(test_datas)\n",
    "        for k,i in zip(a,range(1000)):\n",
    "            xxx[i] = torch.argmax(k)\n",
    "            \n",
    "        return sum(y[int(x)] == 1 for x, y in zip(xxx,self.test_lables))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 10\n",
      "tensor(1.9297, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8809, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3519, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7551, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1638, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9684, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9105, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8171, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5498, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7739, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0472, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8597, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2332, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2678, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4702, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8423, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7323, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1067, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5934, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3727, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9540, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3135, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9009, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9153, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9955, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6893, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8811, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2125, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6110, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8874, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9351, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9150, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9226, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9123, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9067, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9932, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9041, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9464, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8442, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8587, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9594, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6703, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8956, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0730, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9076, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8705, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9141, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8321, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8890, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8963, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0809, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8657, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8617, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9331, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9429, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9399, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7766, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8281, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7594, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9401, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9003, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8107, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9068, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8532, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7935, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8239, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0500, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0699, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6330, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0552, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8746, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1655, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3095, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1458, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9294, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9326, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8121, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8737, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8359, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0362, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9484, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0991, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8198, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8379, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9284, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8047, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7689, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7243, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7436, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9085, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6935, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1013, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8342, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7710, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9372, grad_fn=<NllLossBackward>)\n",
      "0 测试结果: tensor(144)\n",
      "tensor(1.7865, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9071, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0320, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9674, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7807, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8815, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9537, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7466, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9686, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7223, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8378, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8076, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7128, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8774, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6884, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8320, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8901, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4499, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4602, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3904, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5563, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0031, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8147, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7867, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1182, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9689, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9224, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6605, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4363, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0863, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0822, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7544, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6876, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9017, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1684, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0712, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0434, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4565, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7129, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0181, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1793, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7623, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0649, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8813, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7877, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8838, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9960, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8882, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0338, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8054, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8685, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9544, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9375, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9033, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9086, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8665, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8999, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8729, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8619, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8862, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8671, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8826, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9207, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9636, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9496, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9005, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9451, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8305, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8993, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8381, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8615, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8287, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8314, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8465, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9877, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8824, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8814, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9424, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8514, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9050, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8261, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8632, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9493, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8918, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9808, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8205, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8419, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7557, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8615, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9455, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7736, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9981, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7118, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9079, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7823, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9428, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7483, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9404, grad_fn=<NllLossBackward>)\n",
      "1 测试结果: tensor(265)\n",
      "tensor(1.6990, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9423, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7143, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8754, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7097, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0223, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8432, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8241, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9823, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6555, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6941, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7470, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6712, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8051, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7245, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7803, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8081, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9366, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6902, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9286, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8815, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6808, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8056, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8360, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8216, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8759, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8026, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8315, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6777, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7569, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8156, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8096, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7967, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8437, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8595, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8973, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7601, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9554, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0180, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7708, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0776, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9460, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8254, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7110, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6450, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8801, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9259, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8401, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9273, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9133, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8779, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9310, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8513, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8701, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8670, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7741, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9110, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7939, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8383, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8186, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8875, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7708, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8302, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7813, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7609, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7842, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8307, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7570, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7666, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9040, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8805, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7391, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7644, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9669, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0473, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8570, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8351, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7041, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8497, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9505, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8294, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0028, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9097, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0199, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8899, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1502, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8408, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8543, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6847, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9255, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7205, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9237, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9427, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8150, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8457, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0334, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8729, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9153, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7262, grad_fn=<NllLossBackward>)\n",
      "2 测试结果: tensor(265)\n",
      "tensor(1.9243, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7263, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9212, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6778, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8815, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8812, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8169, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0403, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7798, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9137, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7602, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7027, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0180, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8138, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7463, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8763, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7479, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8338, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9089, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8242, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8765, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-67ca2332c7da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFaceNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_lable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_lable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9962ef008d67>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;31m#update model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"测试结果:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = FaceNet(training_data,training_lable,10,10,0.001,test_data, test_lable)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(x):\n",
    "    return [float(a) for a in x[0].split()]\n",
    "\n",
    "data = [to_int(x) for x in data]\n",
    "data = np.array(data)\n",
    "lable = [x[0] for x in lable]\n",
    "lable = np.array(lable)\n",
    "data = [[x.reshape(48,48)] for x in data]\n",
    "data = np.array(datas)\n",
    "datas = torch.from_numpy(datas)\n",
    "lables = torch.from_numpy(lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([10, 1, 48, 48])\n",
      "tensor([[[[ 70.,  80.,  82.,  ...,  52.,  43.,  41.],\n",
      "          [ 65.,  61.,  58.,  ...,  56.,  52.,  44.],\n",
      "          [ 50.,  43.,  54.,  ...,  49.,  56.,  47.],\n",
      "          ...,\n",
      "          [ 91.,  65.,  42.,  ...,  72.,  56.,  43.],\n",
      "          [ 77.,  82.,  79.,  ..., 105.,  70.,  46.],\n",
      "          [ 77.,  72.,  84.,  ..., 106., 109.,  82.]]],\n",
      "\n",
      "\n",
      "        [[[151., 150., 147.,  ..., 129., 140., 120.],\n",
      "          [151., 149., 149.,  ..., 122., 141., 137.],\n",
      "          [151., 151., 156.,  ..., 109., 123., 146.],\n",
      "          ...,\n",
      "          [188., 188., 121.,  ..., 185., 185., 186.],\n",
      "          [188., 187., 196.,  ..., 186., 182., 187.],\n",
      "          [186., 184., 185.,  ..., 193., 183., 184.]]],\n",
      "\n",
      "\n",
      "        [[[231., 212., 156.,  ...,  44.,  27.,  16.],\n",
      "          [229., 175., 148.,  ...,  27.,  35.,  27.],\n",
      "          [214., 156., 157.,  ...,  28.,  22.,  28.],\n",
      "          ...,\n",
      "          [241., 245., 250.,  ...,  57., 101., 146.],\n",
      "          [246., 250., 252.,  ...,  78., 105., 162.],\n",
      "          [250., 251., 250.,  ...,  88., 110., 152.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 77.,  78.,  79.,  ...,  74.,  77.,  76.],\n",
      "          [ 83.,  84.,  82.,  ...,  75.,  79.,  77.],\n",
      "          [ 84.,  87.,  84.,  ...,  74.,  79.,  79.],\n",
      "          ...,\n",
      "          [ 65.,  70.,  66.,  ...,  63.,  77.,  60.],\n",
      "          [ 66.,  69.,  75.,  ...,  45.,  69.,  76.],\n",
      "          [ 69.,  80.,  77.,  ..., 125.,  67.,  68.]]],\n",
      "\n",
      "\n",
      "        [[[ 85.,  84.,  90.,  ...,  40.,  46.,  63.],\n",
      "          [ 78.,  76., 101.,  ...,  36.,  44.,  52.],\n",
      "          [ 76.,  96., 115.,  ...,  34.,  42.,  43.],\n",
      "          ...,\n",
      "          [102., 100.,  97.,  ...,  65.,  76.,  62.],\n",
      "          [107., 101.,  96.,  ...,  57.,  79.,  70.],\n",
      "          [106., 103.,  94.,  ...,  58.,  73.,  84.]]],\n",
      "\n",
      "\n",
      "        [[[255., 254., 255.,  ..., 254., 255., 255.],\n",
      "          [255., 254., 254.,  ..., 253., 255., 255.],\n",
      "          [255., 254., 253.,  ..., 252., 255., 255.],\n",
      "          ...,\n",
      "          [255., 254., 255.,  ..., 254., 255., 255.],\n",
      "          [255., 254., 255.,  ..., 254., 255., 255.],\n",
      "          [255., 254., 255.,  ..., 254., 255., 255.]]]], dtype=torch.float64)\n",
      "torch.Size([10])\n",
      "tensor([0, 0, 2, 4, 6, 2, 4, 3, 3, 2], dtype=torch.int32)\n",
      "> \u001b[1;32m<ipython-input-20-fac7aa7f682a>\u001b[0m(10)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[1;33m\u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      9 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 10 \u001b[1;33m\u001b[1;32mfor\u001b[0m \u001b[0mmini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatas_0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     11 \u001b[1;33m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     12 \u001b[1;33m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datas = datas.double()\n",
    "datas = datas[:20000]\n",
    "lables = lables[:20000]\n",
    "\n",
    "import pdb\n",
    "\n",
    "for mini, x, y in datas_0:\n",
    "    print(mini)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(x):\n",
    "    return [float(a) for a in x[0].split()]\n",
    "\n",
    "data = [to_int(x) for x in data]\n",
    "data = np.array(data)\n",
    "lable = [x[0] for x in lable]\n",
    "lable = np.array(lable)\n",
    "datas = [[x.reshape(48,48)] for x in data]\n",
    "datas = np.array(datas)\n",
    "datas = torch.from_numpy(datas)\n",
    "lables = torch.from_numpy(lable)\n",
    "datas = datas[:20000]\n",
    "lables = lables[:20000]\n",
    "datas_0 = [\n",
    "[n,datas[k:k+10],lables[k:k+10]]\n",
    "for n,k,l in zip(range(2000),range(0, n, 10),range(0, n, 10))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
