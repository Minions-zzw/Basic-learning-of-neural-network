{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import gzip\n",
    "from PIL import Image\n",
    "def load_data_image():\n",
    "    #读取文件\n",
    "    binfile = open('../data/train-images.idx3-ubyte', 'rb')\n",
    "    buffers = binfile.read()\n",
    "    #/读取前四个无符号整数\n",
    "    magic, num, cows, cols = struct.unpack_from('>IIII',buffers,0)\n",
    "    #获得偏移\n",
    "    offset = struct.calcsize('>IIII')\n",
    "    #计算所读取的数据大小num*cows*cols个bit，>表示大端读取\n",
    "    strString = '>' + str(num*cows*cols) + 'B'\n",
    "    #读取图片数据并转化矩阵\n",
    "    training_image = struct.unpack_from(strString, buffers, offset)\n",
    "    training_image = np.reshape(training_image,(num,cows*cols))\n",
    "    binfile.close()\n",
    "    return (training_image)\n",
    "def load_data_lable():\n",
    "    #读取文件\n",
    "    binfile = open('../data/train-labels.idx1-ubyte', 'rb')\n",
    "    buffers = binfile.read()\n",
    "    #/读取前两个无符号整数\n",
    "    magic, num = struct.unpack_from('>II',buffers,0)\n",
    "    #获得偏移\n",
    "    offset = struct.calcsize('>II')\n",
    "    #计算所读取的数据大小num*cows*cols个bit，>表示大端读取\n",
    "    strString = '>' + str(num) + 'B'\n",
    "    #读取图片数据并转化矩阵\n",
    "    training_lable = struct.unpack_from(strString, buffers, offset)\n",
    "    training_lable = np.reshape(training_lable,(num))\n",
    "    binfile.close()\n",
    "    return (training_lable)\n",
    "def I(z):#S型神经元\n",
    "    for x in range(784):\n",
    "        z[x] = z[x]>0\n",
    "    return z\n",
    "training_image = load_data_image()\n",
    "training_lable = load_data_lable()\n",
    "training_lables = np.zeros((60000,10))\n",
    "\n",
    "for n,x in zip(range(60000),training_lable):\n",
    "    training_lables[n][x] = 1\n",
    "training_lable = training_lables\n",
    "\n",
    "for x,i in zip(training_image,range(60000)):\n",
    "    training_image[i] = I(x)\n",
    "    \n",
    "test_image = training_image[50000:]\n",
    "test_lable = training_lable[50000:]\n",
    "training_image = training_image[:50000]\n",
    "training_lable = training_lable[:50000]\n",
    "training_data = zip(training_image,training_lable)\n",
    "test_data = zip(test_image,test_lable)\n",
    "training_data = list(training_data)\n",
    "test_data = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class Network(object):\n",
    "    def __init__(self,size):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(size[0], size[1], bias = False), # w_1 * x +b_1\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size[1], size[2] , bias = False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(size[2], size[3] , bias = False),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss(reduction = 'sum')\n",
    "        \n",
    "    def run(self,training_data,epochs,mini_batch_size,eta,test_data = None):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = eta)\n",
    "        \n",
    "        if test_data: \n",
    "            test_data = list(test_data)\n",
    "            test_data = test_data[:1000]\n",
    "            n_test = len(test_data)\n",
    "        training_data = list(training_data)\n",
    "        \n",
    "#         training_data = training_data[:1000] #可以改变训练集大小，测试使用\n",
    "        \n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            k = len(mini_batches)\n",
    "            for  mini_batch in mini_batches:\n",
    "                loss = self.training(mini_batch,mini_batch_size)\n",
    "            print('LOSS {0},'.format(loss))\n",
    "            if j >= 0:\n",
    "                print('Epoch {0}:{1} / {2}'. format(\n",
    "                        j, self.test(test_data), n_test))\n",
    "    def training(self, training_data, size):\n",
    "        x = torch.zeros((size,784),dtype = torch.float)\n",
    "        y = torch.zeros((size,10), dtype = torch.float)\n",
    "        for (a, b), i in zip(training_data, range(size)):\n",
    "            x[i] = torch.from_numpy(a) #转换成二维的矩阵，后面做乘法方便\n",
    "            y[i] = torch.from_numpy(b)\n",
    "        #Forward pass\n",
    "        y_pred = self.model(x) # model.forward()\n",
    "        #loss\n",
    "        loss = self.loss_fn(y_pred , y) #computation graph  \n",
    "        \n",
    "        #重置参数增加量\n",
    "        self.optimizer.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "            \n",
    "        #update model parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    def test(self,test_data):\n",
    "        test_results = [(torch.argmax(self.model(torch.from_numpy(x).reshape(1,784).float())), torch.from_numpy(y))\n",
    "                for x, y in test_data]\n",
    "        return sum(y[int(x)] == 1 for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 1.0558489561080933,\n",
      "Epoch 0:888 / 1000\n",
      "LOSS 0.34584712982177734,\n",
      "Epoch 1:916 / 1000\n",
      "LOSS 0.25051459670066833,\n",
      "Epoch 2:930 / 1000\n",
      "LOSS 0.24110911786556244,\n",
      "Epoch 3:940 / 1000\n",
      "LOSS 0.27753961086273193,\n",
      "Epoch 4:948 / 1000\n",
      "LOSS 0.2930174171924591,\n",
      "Epoch 5:955 / 1000\n",
      "LOSS 0.2592984139919281,\n",
      "Epoch 6:959 / 1000\n",
      "LOSS 0.21915581822395325,\n",
      "Epoch 7:960 / 1000\n",
      "LOSS 0.19306927919387817,\n",
      "Epoch 8:961 / 1000\n",
      "LOSS 0.14170879125595093,\n",
      "Epoch 9:964 / 1000\n",
      "LOSS 0.11819624155759811,\n",
      "Epoch 10:964 / 1000\n",
      "LOSS 0.07775414735078812,\n",
      "Epoch 11:964 / 1000\n",
      "LOSS 0.05686663091182709,\n",
      "Epoch 12:964 / 1000\n",
      "LOSS 0.03964913636445999,\n",
      "Epoch 13:963 / 1000\n",
      "LOSS 0.01957719214260578,\n",
      "Epoch 14:965 / 1000\n",
      "LOSS 0.011769394390285015,\n",
      "Epoch 15:962 / 1000\n",
      "LOSS 0.0069373007863759995,\n",
      "Epoch 16:964 / 1000\n",
      "LOSS 0.0033634183928370476,\n",
      "Epoch 17:966 / 1000\n",
      "LOSS 0.0023583746515214443,\n",
      "Epoch 18:967 / 1000\n",
      "LOSS 0.0012901362497359514,\n",
      "Epoch 19:968 / 1000\n",
      "LOSS 0.0008566924370825291,\n",
      "Epoch 20:966 / 1000\n",
      "LOSS 0.0005064915749244392,\n",
      "Epoch 21:966 / 1000\n",
      "LOSS 0.00043476169230416417,\n",
      "Epoch 22:965 / 1000\n",
      "LOSS 0.00012487443746067584,\n",
      "Epoch 23:965 / 1000\n",
      "LOSS 8.86544949025847e-05,\n",
      "Epoch 24:968 / 1000\n",
      "LOSS 5.184425026527606e-05,\n",
      "Epoch 25:967 / 1000\n",
      "LOSS 0.00020663702161982656,\n",
      "Epoch 26:964 / 1000\n",
      "LOSS 4.19709540437907e-05,\n",
      "Epoch 27:962 / 1000\n",
      "LOSS 4.333654578658752e-05,\n",
      "Epoch 28:967 / 1000\n",
      "LOSS 1.5676880138926208e-05,\n",
      "Epoch 29:968 / 1000\n",
      "LOSS 5.822110688313842e-05,\n",
      "Epoch 30:964 / 1000\n",
      "LOSS 1.3730554201174527e-05,\n",
      "Epoch 31:968 / 1000\n",
      "LOSS 5.765293735748855e-06,\n",
      "Epoch 32:964 / 1000\n",
      "LOSS 6.085136192268692e-06,\n",
      "Epoch 33:967 / 1000\n",
      "LOSS 2.831311576301232e-06,\n",
      "Epoch 34:966 / 1000\n",
      "LOSS 8.70457552082371e-06,\n",
      "Epoch 35:961 / 1000\n",
      "LOSS 1.935316277013044e-06,\n",
      "Epoch 36:968 / 1000\n",
      "LOSS 0.000340764265274629,\n",
      "Epoch 37:961 / 1000\n",
      "LOSS 3.045549192393082e-06,\n",
      "Epoch 38:967 / 1000\n",
      "LOSS 3.6727237784361932e-06,\n",
      "Epoch 39:966 / 1000\n",
      "LOSS 1.918665049061019e-07,\n",
      "Epoch 40:961 / 1000\n",
      "LOSS 6.994488330747117e-07,\n",
      "Epoch 41:964 / 1000\n",
      "LOSS 2.201695679104887e-06,\n",
      "Epoch 42:966 / 1000\n",
      "LOSS 2.605711983960646e-07,\n",
      "Epoch 43:964 / 1000\n",
      "LOSS 5.07972072227858e-06,\n",
      "Epoch 44:962 / 1000\n",
      "LOSS 1.59171474933828e-06,\n",
      "Epoch 45:969 / 1000\n",
      "LOSS 3.113502202722884e-07,\n",
      "Epoch 46:967 / 1000\n",
      "LOSS 3.0132731865251117e-08,\n",
      "Epoch 47:963 / 1000\n",
      "LOSS 9.323269267724754e-08,\n",
      "Epoch 48:969 / 1000\n",
      "LOSS 1.4640668268839363e-05,\n",
      "Epoch 49:962 / 1000\n",
      "LOSS 2.3685016614649612e-08,\n",
      "Epoch 50:969 / 1000\n",
      "LOSS 2.391942199864161e-09,\n",
      "Epoch 51:969 / 1000\n",
      "LOSS 8.630426151512438e-08,\n",
      "Epoch 52:965 / 1000\n",
      "LOSS 6.475335112554603e-07,\n",
      "Epoch 53:965 / 1000\n",
      "LOSS 9.818259094629411e-09,\n",
      "Epoch 54:964 / 1000\n",
      "LOSS 5.776648137612028e-08,\n",
      "Epoch 55:969 / 1000\n",
      "LOSS 1.3148571120780161e-08,\n",
      "Epoch 56:968 / 1000\n",
      "LOSS 3.3164880619551695e-08,\n",
      "Epoch 57:967 / 1000\n",
      "LOSS 1.9541738893735783e-08,\n",
      "Epoch 58:966 / 1000\n",
      "LOSS 2.4400153506576316e-06,\n",
      "Epoch 59:965 / 1000\n",
      "LOSS 6.906788030391908e-08,\n",
      "Epoch 60:964 / 1000\n",
      "LOSS 5.030053529253564e-08,\n",
      "Epoch 61:963 / 1000\n",
      "LOSS 3.051914632123953e-08,\n",
      "Epoch 62:958 / 1000\n",
      "LOSS 2.9043674132367414e-08,\n",
      "Epoch 63:969 / 1000\n",
      "LOSS 2.235495166758028e-08,\n",
      "Epoch 64:967 / 1000\n",
      "LOSS 1.5572738654157092e-09,\n",
      "Epoch 65:961 / 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-0232f52e8bdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-04f8792d8b87>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mfor\u001b[0m  \u001b[0mmini_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOSS {0},'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-04f8792d8b87>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(self, training_data, size)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#update model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Network([784,100,50,10])\n",
    "net.run(training_data,100,5,0.0001,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
