{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def load_data():\n",
    "    #读取数据\n",
    "    import json\n",
    "    datafile = 'work\\housing.data'\n",
    "    data = np.fromfile(datafile,sep = ' ')\n",
    "    #把数据转换成每行14个元素的矩阵\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                     'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "    #设置训练数据和测试数据\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0]*ratio) #取得数据量百分之八十的位置\n",
    "    training_data = data[:offset] #读取前offset个数据\n",
    "     #对数据进行归一化处理\n",
    "    maximums,minimums,avgs=\\\n",
    "            training_data.max(axis=0),\\\n",
    "            training_data.min(axis=0),\\\n",
    "    training_data.sum(axis=0)/training_data.shape[0]\n",
    "    for i in range(feature_num):\n",
    "        data[:,i] = (data[:,i] - avgs[i]) / (maximums[i] - minimums[i])\n",
    "        \n",
    "    \n",
    "    train_data = data[:offset]#读取前offset个数据\n",
    "    test_data = data[offset:]#读取剩下的数据\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_data,test_data = load_data() #读取文件函数，上面已经定义了\n",
    "x = train_data[:,:-1] #把数据的前13个参数给x，房子的信息参数\n",
    "y = train_data[:,-1:] #把最后一个参数给y，真实房价\n",
    "#torch方法做\n",
    "a = torch.from_numpy(x) #用torch来做，这一步是转换数据类型，把numpy的array转换成torch的tenser\n",
    "b = torch.from_numpy(y)\n",
    "a.requires_grad = True #设置a和b的grad，用来计算backforword\n",
    "b.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,-0.1,-0.2,-0.3,-0.4,0.0] #测试\n",
    "w = np.array(w).reshape([13,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights): #num_of_weights-参数的维度\n",
    "        #随机产生w的初始值\n",
    "        #为了保持程序每次运行的一致性，\n",
    "        #此处设置固定的随机数种子\n",
    "        np.random.seed(0) #设置随机数种子\n",
    "        self.w = np.random.randn(num_of_weights, 1) #声明有13个参数的随机的参数表\n",
    "        self.b = 0. #声明附加参数\n",
    "        \n",
    "    def forward(self, x): #x - 训练数据，维度和num_of_weights一致\n",
    "        z = np.dot(x, self.w) + self.b #计算forward—x.dot(self.w)—用x矩阵和参数w矩阵进行矩阵乘法运算 再加上附加参数\n",
    "        return z\n",
    "    def loss(self,z,y): #计算loss，z-计算出的forword数据，y-真实数据\n",
    "        error = z-y #loss减去真实数据\n",
    "        cost = np.square(error) #平方\n",
    "        cost = np.mean(cost)  #取平均数\n",
    "        return cost\n",
    "    def gradient(self,x,y): #计算参数的grad，x-训练数据，维度和num_of_weights一致,y-真实数据\n",
    "        z = self.forward(x) #forword计算\n",
    "        gradient_w = (z-y)*x #反向求导\n",
    "        gradient_w = np.mean(gradient_w, axis = 0)#压缩列，取所有列的平均值\n",
    "        gradient_w = gradient_w[:,np.newaxis] #重新设置成二维矩阵，不然无法进行操作\n",
    "        gradient_b = (z-y) #求附加参数的偏导\n",
    "        gradient_b = np.mean(gradient_b) #取平均值\n",
    "        \n",
    "        return gradient_w, gradient_b #返回求出的w_grad和b_grad,\n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01): #更新参数，gradient_w参数w的变化率，gradient_b-参数b的变化率\n",
    "        self.w = self.w - eta * gradient_w #用原来的参数w减去w_grad，用以更新参数w\n",
    "        self.b = self.b - eta * gradient_b #用原来的参数b减去b_grad，用以更新参数b\n",
    "    def train(self, x, y, iterations = 100, eta = 0.01): #训练函数 iterations-训练次数 eta-参数改变幅度\n",
    "        losses = [] #losses表，用于记录所有训练的loss值\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x) #计算forword\n",
    "            L = self.loss(z,y) #计算loss\n",
    "            gradient_w, gradient_b = self.gradient(x, y) #计算w_grad和b_grad\n",
    "            self.update(gradient_w, gradient_b, eta) #用w_grad和b_grad更新参数w和参数b\n",
    "            losses.append(L) #将loss加入loss表\n",
    "#             if(i + 1)%10 == 0:#每十次训练查看一次loss\n",
    "#                 print('iter{},loss{}'.format(i,L))\n",
    "        return losses #返回losses表\n",
    "    \n",
    "    \n",
    "#torch方法做\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Network1(torch.nn.Module):\n",
    "    def __init__(self, num_of_weights):\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.a = torch.randn(num_of_weights, 1,dtype=torch.float64,requires_grad = True)\n",
    "        self.b = 0.\n",
    "        \n",
    "#         super(Network1, self).__init__()\n",
    "#         torch.manual_seed(0)\n",
    "# #         self.linear1 = torch.randn(num_of_weights,1, bias = False)\n",
    "# #         self.linear2 = 0.\n",
    "#         self.linear1 = torch.nn.Linear(num_of_weights,1, bias = False)\n",
    "#         self.linear2 = 0.\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = x.mm(self.a)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7832ccdf4ece>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#    -1.1807961 ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#读取文件函数，上面已经定义了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#把数据的前13个参数给x，房子的信息参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# net = Network(13)\n",
    "# x1 = x[:3]\n",
    "# y1 = y[:3]\n",
    "# z = net.forward(x1)\n",
    "# print(z)\n",
    "# print(y1)\n",
    "# loss = net.loss(z,y1)\n",
    "# # print(loss)\n",
    "\n",
    "\n",
    "# gradient_w = 2 *(z - y1)* x1\n",
    "# print('gradient_w', gradient_w.shape)\n",
    "# gradient_w = np.mean(gradient_w, axis = 0)\n",
    "# gradient_w = gradient_w[:,np.newaxis]\n",
    "# print('gradient_w', gradient_w.shape)\n",
    "# print(gradient_w)\n",
    "\n",
    "# gradient_b = (z-y1)\n",
    "# gradient_b = np.mean(gradient_b)\n",
    "\n",
    "# gradient_b\n",
    "\n",
    "\n",
    "# eta = 0.1\n",
    "# net.w[5] = net.w[5]-eta* gradient_w5\n",
    "# net.w[9] = net.w[9]-eta* gradient_w9\n",
    "#计算Z和loss\n",
    "\n",
    "\n",
    "\n",
    "# gradient_w5 = gradient_w[5][0]\n",
    "# gradient_w9 = gradient_w[9][0]\n",
    "# print(net.w[5][0],net.w[9][0], loss)\n",
    "# print(gradient_w5,gradient_w9)\n",
    "\n",
    "# [ -0.14407246   0.25288296  -1.91658252 ...  -1.94680358   0.34845553\n",
    "#    -1.1807961 ]\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "train_data,test_data = load_data() #读取文件函数，上面已经定义了\n",
    "x = train_data[:,:-1] #把数据的前13个参数给x，房子的信息参数\n",
    "y = train_data[:,-1:] #把最后一个参数给y，真实房价\n",
    "#torch方法做\n",
    "a = torch.from_numpy(x) #用torch来做，这一步是转换数据类型，把numpy的array转换成torch的tenser\n",
    "b = torch.from_numpy(y)\n",
    "a.requires_grad = True #设置a和b的grad，用来计算backforword\n",
    "b.requires_grad = True\n",
    "\n",
    "net = Network(13)\n",
    "num_iterations = 10000\n",
    "#训练\n",
    "losses = net.train(x, y, iterations = num_iterations, eta = 0.01)\n",
    "#画出图表\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x,plot_y)\n",
    "plt.show()\n",
    "\n",
    "x = test_data[:,:-1]\n",
    "y = test_data[:,-1:]\n",
    "z = net.forward(x)\n",
    "L = net.loss(z,y)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
